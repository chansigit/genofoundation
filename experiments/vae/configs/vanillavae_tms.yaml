# Vanilla VAE Training Configuration for TMS Dataset
#
# Usage:
#   # Full dataset (default)
#   python train_vanillavae.py
#
#   # Mini dataset
#   python train_vanillavae.py 'data.train_path=${hydra:runtime.cwd}/../../data/tms/tms_preprocessed-mini.pt'
#
#   # Override other params
#   python train_vanillavae.py trainer.epochs=100 trainer.batch_size=128

# =============================================================================
# Experiment Settings
# =============================================================================
experiment:
  name: "vanillavae_tms"
  seed: 42
  output_dir: "${hydra:runtime.cwd}/../../outputs/vae_tms"

# =============================================================================
# Model Settings
# =============================================================================
model:
  latent_dim: 128

# =============================================================================
# Data Settings
# =============================================================================
data:
  train_path: "${hydra:runtime.cwd}/../../data/tms/tms_preprocessed.pt"
  num_workers: 4
  pin_memory: true

# =============================================================================
# Trainer Settings
# =============================================================================
trainer:
  epochs: 500
  batch_size: 256
  learning_rate: 0.001
  weight_decay: 0.00001
  # VAE beta scheduling
  beta: 1.0
  beta_schedule: "linear"
  beta_warmup_epochs: 10
  # Optimizer
  optimizer: "adamw"
  betas: [0.9, 0.999]
  # LR scheduler
  scheduler: "warmup"
  lr_warmup_epochs: 10
  min_lr: 0.000001
  warmup_start_factor: 0.01
  # Gradient settings
  grad_clip_norm: 50.0
  gradient_accumulation_steps: 1
  record_grad_norm: true
  # Early stopping
  early_stopping: true
  patience: 20
  min_delta: 0.0001
  # Checkpointing
  save_every: 10
  save_best: true
  max_checkpoints: 3
  save_after_epoch: 20  # Skip saving during warmup
  # Logging
  log_every: 100
  wandb_mode: "disabled"
  wandb_project: "vae_tms"
  # Mixed precision
  mixed_precision: "no"
